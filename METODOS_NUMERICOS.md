# ğŸ“ GuÃ­a Completa de MÃ©todos NumÃ©ricos

## Tabla de Contenidos
- [CapÃ­tulo 1: SoluciÃ³n de Ecuaciones No Lineales](#capÃ­tulo-1-soluciÃ³n-de-ecuaciones-no-lineales)
  - [MÃ©todo de BisecciÃ³n](#1-mÃ©todo-de-bisecciÃ³n)
  - [MÃ©todo de Regla Falsa (Falsa PosiciÃ³n)](#2-mÃ©todo-de-regla-falsa-falsa-posiciÃ³n)
  - [MÃ©todo de Punto Fijo](#3-mÃ©todo-de-punto-fijo)
  - [MÃ©todo de Newton-Raphson](#4-mÃ©todo-de-newton-raphson)
  - [MÃ©todo de la Secante](#5-mÃ©todo-de-la-secante)
  - [MÃ©todo de RaÃ­ces MÃºltiples](#6-mÃ©todo-de-raÃ­ces-mÃºltiples)
- [CapÃ­tulo 2: Sistemas de Ecuaciones Lineales](#capÃ­tulo-2-sistemas-de-ecuaciones-lineales)
  - [MÃ©todo de Jacobi](#7-mÃ©todo-de-jacobi)
  - [MÃ©todo de Gauss-Seidel](#8-mÃ©todo-de-gauss-seidel)
  - [MÃ©todo SOR](#9-mÃ©todo-sor-successive-over-relaxation)
- [CapÃ­tulo 3: InterpolaciÃ³n](#capÃ­tulo-3-interpolaciÃ³n)
  - [InterpolaciÃ³n de Vandermonde](#10-interpolaciÃ³n-de-vandermonde)
  - [InterpolaciÃ³n de Newton](#11-interpolaciÃ³n-de-newton)
  - [InterpolaciÃ³n de Lagrange](#12-interpolaciÃ³n-de-lagrange)
  - [Spline Lineal](#13-spline-lineal)
  - [Spline CÃºbico](#14-spline-cÃºbico)
- [ComparaciÃ³n de MÃ©todos](#comparaciÃ³n-de-mÃ©todos)

---

# CapÃ­tulo 1: SoluciÃ³n de Ecuaciones No Lineales

Estos mÃ©todos buscan encontrar las **raÃ­ces** (o **ceros**) de una funciÃ³n f(x), es decir, valores de x donde f(x) = 0.

## 1. MÃ©todo de BisecciÃ³n

### ğŸ¯ Â¿QuÃ© es?
El mÃ©todo de bisecciÃ³n es el mÃ©todo mÃ¡s simple y robusto para encontrar raÃ­ces. Se basa en el **Teorema del Valor Intermedio**: si f(x) es continua en [a,b] y f(a)Â·f(b) < 0, entonces existe al menos una raÃ­z en ese intervalo.

### ğŸ“Š Â¿CÃ³mo funciona?
1. **Inicio**: Necesitas dos puntos `a` y `b` donde f(a) y f(b) tengan **signos opuestos**
2. **IteraciÃ³n**: 
   - Calcula el punto medio: `xm = (a + b) / 2`
   - EvalÃºa f(xm)
   - Si f(a)Â·f(xm) < 0, la raÃ­z estÃ¡ en [a, xm], entonces b = xm
   - Si f(xm)Â·f(b) < 0, la raÃ­z estÃ¡ en [xm, b], entonces a = xm
3. **Repetir** hasta que el intervalo sea suficientemente pequeÃ±o

### âœ… Ventajas
- **Siempre converge** si se cumplen las condiciones iniciales
- **Muy robusto** y simple de implementar
- **No requiere derivadas**
- Error garantizado: se reduce a la mitad en cada iteraciÃ³n

### âŒ Desventajas
- **Convergencia lenta** (lineal): cada iteraciÃ³n solo reduce el error a la mitad
- **Requiere intervalo inicial** donde la funciÃ³n cambie de signo
- No puede encontrar raÃ­ces de multiplicidad par (donde la funciÃ³n toca pero no cruza el eje x)

### ğŸ”´ CuÃ¡ndo falla
- Si f(a) y f(b) tienen el **mismo signo**
- Si hay **mÃºltiples raÃ­ces** en el intervalo (solo encontrarÃ¡ una)
- Si la funciÃ³n tiene **discontinuidades** en el intervalo

### ğŸ’¡ CuÃ¡ndo usar
- Cuando necesitas **garantÃ­a de convergencia**
- Como **mÃ©todo de respaldo** cuando otros mÃ©todos fallan
- Para **acotar** inicialmente donde estÃ¡ la raÃ­z antes de usar un mÃ©todo mÃ¡s rÃ¡pido
- Cuando **no tienes** o no puedes calcular la derivada

### ğŸ“ˆ Ejemplo visual
```
IteraciÃ³n 0: [2.0000, 3.0000] â†’ xm = 2.5000
IteraciÃ³n 1: [2.0000, 2.5000] â†’ xm = 2.2500
IteraciÃ³n 2: [2.0000, 2.2500] â†’ xm = 2.1250
...converge lentamente pero seguro
```

---

## 2. MÃ©todo de Regla Falsa (Falsa PosiciÃ³n)

### ğŸ¯ Â¿QuÃ© es?
Similar a bisecciÃ³n, pero en lugar de usar el punto medio, usa **interpolaciÃ³n lineal** para estimar dÃ³nde la funciÃ³n cruza el eje x.

### ğŸ“Š Â¿CÃ³mo funciona?
1. **Inicio**: Igual que bisecciÃ³n: puntos `a` y `b` con f(a)Â·f(b) < 0
2. **IteraciÃ³n**:
   - En lugar del punto medio, calcula: `c = b - f(b)Â·(b-a)/(f(b)-f(a))`
   - Esta fÃ³rmula traza una lÃ­nea recta entre (a, f(a)) y (b, f(b))
   - Encuentra donde esa lÃ­nea cruza el eje x
3. **Actualiza** el intervalo igual que bisecciÃ³n

### âœ… Ventajas
- **Generalmente mÃ¡s rÃ¡pido** que bisecciÃ³n
- **Siempre converge** con las condiciones adecuadas
- **No requiere derivadas**
- Usa informaciÃ³n de los valores de la funciÃ³n (no solo los signos)

### âŒ Desventajas
- Puede ser **mÃ¡s lento que bisecciÃ³n** en algunos casos
- Un extremo del intervalo puede **"quedarse pegado"** (convergencia unilateral)
- MÃ¡s complejo que bisecciÃ³n

### ğŸ”´ CuÃ¡ndo falla
- Mismas condiciones que bisecciÃ³n
- Puede converger muy lentamente si la funciÃ³n es muy **cÃ³ncava o convexa**

### ğŸ’¡ CuÃ¡ndo usar
- Cuando quieres algo **mÃ¡s rÃ¡pido que bisecciÃ³n** pero igual de seguro
- Cuando la funciÃ³n es **aproximadamente lineal** en el intervalo
- Como alternativa a bisecciÃ³n cuando quieres aprovechar los valores de f(x)

### ğŸ“ˆ ComparaciÃ³n con BisecciÃ³n
```
BisecciÃ³n:  usa (a+b)/2     â†’ corta en el medio
Regla Falsa: usa interpolaciÃ³n â†’ corta donde "deberÃ­a" estar la raÃ­z
```

---

## 3. MÃ©todo de Punto Fijo

### ğŸ¯ Â¿QuÃ© es?
Transforma f(x) = 0 en una ecuaciÃ³n equivalente **x = g(x)**, y busca el punto donde x es igual a g(x).

### ğŸ“Š Â¿CÃ³mo funciona?
1. **TransformaciÃ³n**: Convierte f(x) = 0 en x = g(x)
   - Ejemplo: si f(x) = xÂ² - 5, puedes usar g(x) = âˆš5
2. **IteraciÃ³n**: 
   - Comienza con un valor inicial xâ‚€
   - Calcula: xâ‚ = g(xâ‚€), xâ‚‚ = g(xâ‚), xâ‚ƒ = g(xâ‚‚), ...
3. **Converge** si |g'(x)| < 1 cerca de la raÃ­z

### âœ… Ventajas
- **Muy simple** de implementar
- **No requiere derivadas** de f(x) (pero sÃ­ de g(x) para anÃ¡lisis)
- Puede ser **muy rÃ¡pido** si se elige bien g(x)
- Ãštil para sistemas de ecuaciones

### âŒ Desventajas
- **No siempre converge**: depende de la elecciÃ³n de g(x)
- Requiere anÃ¡lisis previo para verificar convergencia
- Puede **diverger** rÃ¡pidamente si g'(x) > 1
- La elecciÃ³n de g(x) no es Ãºnica ni obvia

### ğŸ”´ CuÃ¡ndo falla
- Si **|g'(x)| â‰¥ 1** en la regiÃ³n de la raÃ­z â†’ diverge
- Si xâ‚€ estÃ¡ **muy lejos** de la raÃ­z
- Si g(x) estÃ¡ **mal elegida**

### ğŸ’¡ CuÃ¡ndo usar
- Cuando puedes encontrar una buena funciÃ³n g(x)
- Para ecuaciones que **naturalmente** se expresan como x = g(x)
- Cuando conoces la **regiÃ³n aproximada** de la raÃ­z
- En sistemas de ecuaciones no lineales

### ğŸ“ˆ Criterio de convergencia
```
|g'(x)| < 1  â†’ Converge
|g'(x)| = 1  â†’ Puede o no converger
|g'(x)| > 1  â†’ Diverge
```

### ğŸ¨ Ejemplo
Para f(x) = xÂ³ - 2x - 5 = 0, puedes usar:
- gâ‚(x) = (2x + 5)^(1/3)  â†’ FUNCIONA si |g'(x)| < 1
- gâ‚‚(x) = (xÂ³ - 5)/2      â†’ Puede NO funcionar

---

## 4. MÃ©todo de Newton-Raphson

### ğŸ¯ Â¿QuÃ© es?
El mÃ©todo mÃ¡s popular y eficiente. Usa la **tangente** de la funciÃ³n para aproximarse rÃ¡pidamente a la raÃ­z.

### ğŸ“Š Â¿CÃ³mo funciona?
1. **FÃ³rmula**: x_(n+1) = x_n - f(x_n)/f'(x_n)
2. **GeomÃ©tricamente**: 
   - Dibuja la tangente a la curva en (x_n, f(x_n))
   - El siguiente punto x_(n+1) es donde esa tangente cruza el eje x
3. **Repetir** hasta converger

### âœ… Ventajas
- **Convergencia cuadrÃ¡tica**: el error se reduce al cuadrado en cada iteraciÃ³n
- **Muy rÃ¡pido** cuando funciona
- Solo necesita **un punto inicial**
- Ampliamente usado y estudiado

### âŒ Desventajas
- **Requiere la derivada** f'(x)
- Puede **diverger** si xâ‚€ estÃ¡ mal elegido
- Falla si f'(x) = 0 en algÃºn punto
- Sensible a la elecciÃ³n de xâ‚€

### ğŸ”´ CuÃ¡ndo falla
- Si **f'(x) = 0** o muy pequeÃ±a â†’ divisiÃ³n por cero o valores enormes
- Si xâ‚€ estÃ¡ en un **mÃ¡ximo o mÃ­nimo local**
- Si xâ‚€ estÃ¡ **muy lejos** de la raÃ­z
- En funciones con **mÃºltiples raÃ­ces** cercanas
- En puntos de **inflexiÃ³n** puede oscilar

### ğŸ’¡ CuÃ¡ndo usar
- Cuando tienes una **buena aproximaciÃ³n inicial**
- Cuando puedes **calcular la derivada** fÃ¡cilmente
- Cuando necesitas **alta precisiÃ³n rÃ¡pidamente**
- Para funciones **suaves** y bien comportadas

### ğŸ“ˆ Convergencia
```
Error inicial: 10^-1
DespuÃ©s de 1 iter: 10^-2  (cuadrÃ¡tica)
DespuÃ©s de 2 iter: 10^-4
DespuÃ©s de 3 iter: 10^-8
DespuÃ©s de 4 iter: 10^-16 (Â¡ya convergiÃ³!)
```

### ğŸ¨ VisualizaciÃ³n
```
f(x) 
  |     /
  |    /  â† tangente en xâ‚€
  |   /
  |  â€¢------ xâ‚€
  | /
  |/___â€¢____ xâ‚  (donde tangente cruza eje x)
  |      
```

---

## 5. MÃ©todo de la Secante

### ğŸ¯ Â¿QuÃ© es?
Es una **variante de Newton** que aproxima la derivada usando diferencias finitas. No requiere calcular f'(x) analÃ­ticamente.

### ğŸ“Š Â¿CÃ³mo funciona?
1. **Inicio**: Necesitas **dos puntos iniciales** xâ‚€ y xâ‚ (no necesitan tener signos opuestos)
2. **FÃ³rmula**: x_(n+1) = x_n - f(x_n)Â·(x_n - x_(n-1))/(f(x_n) - f(x_(n-1)))
3. **AproximaciÃ³n**: Usa (f(x_n) - f(x_(n-1)))/(x_n - x_(n-1)) â‰ˆ f'(x_n)

### âœ… Ventajas
- **No requiere derivadas** (solo evaluaciones de f)
- **MÃ¡s rÃ¡pido que bisecciÃ³n**
- Convergencia superlineal (orden â‰ˆ 1.618, el nÃºmero Ã¡ureo Ï†)
- MÃ¡s robusto que Newton en algunos casos

### âŒ Desventajas
- **Requiere dos puntos iniciales**
- **MÃ¡s lento que Newton** (pero mÃ¡s rÃ¡pido que bisecciÃ³n)
- Puede fallar si f(x_n) - f(x_(n-1)) â‰ˆ 0
- No garantiza convergencia

### ğŸ”´ CuÃ¡ndo falla
- Si **f(x_n) = f(x_(n-1))** â†’ divisiÃ³n por cero
- Si los dos puntos iniciales estÃ¡n **mal elegidos**
- En funciones con **comportamiento errÃ¡tico**

### ğŸ’¡ CuÃ¡ndo usar
- Cuando **no puedes o no quieres calcular la derivada**
- Como alternativa a Newton cuando f'(x) es compleja
- Cuando tienes dos **aproximaciones razonables** de la raÃ­z
- Para funciones **definidas numÃ©ricamente** (no analÃ­ticamente)

### ğŸ“ˆ ComparaciÃ³n
```
BisecciÃ³n:     Convergencia lineal    (orden 1)
Secante:       Convergencia superlineal (orden 1.618)
Newton:        Convergencia cuadrÃ¡tica (orden 2)
```

---

## 6. MÃ©todo de RaÃ­ces MÃºltiples

### ğŸ¯ Â¿QuÃ© es?
Una **modificaciÃ³n del mÃ©todo de Newton** para encontrar raÃ­ces con **multiplicidad mayor que 1** (donde f(x) = f'(x) = 0).

### ğŸ“Š Â¿CÃ³mo funciona?
1. **Problema**: Newton estÃ¡ndar converge linealmente en raÃ­ces mÃºltiples
2. **SoluciÃ³n**: Usa la fÃ³rmula modificada:
   ```
   x_(n+1) = x_n - mÂ·f(x_n)/f'(x_n)
   ```
   donde m es la **multiplicidad** de la raÃ­z
3. **Alternativa** (si no conoces m):
   ```
   x_(n+1) = x_n - f(x_n)Â·f'(x_n) / [f'(x_n)Â² - f(x_n)Â·f''(x_n)]
   ```

### âœ… Ventajas
- **Recupera convergencia cuadrÃ¡tica** en raÃ­ces mÃºltiples
- Funciona donde Newton estÃ¡ndar es lento
- AutomÃ¡tico si usas la segunda fÃ³rmula

### âŒ Desventajas
- Requiere **segunda derivada** f''(x)
- MÃ¡s **costoso computacionalmente**
- Puede ser inestable si f''(x) es grande
- Necesitas saber si hay multiplicidad (a veces)

### ğŸ”´ CuÃ¡ndo falla
- Si **f''(x)** no estÃ¡ disponible o es difÃ­cil de calcular
- Si los cÃ¡lculos numÃ©ricos introducen **errores de redondeo** significativos
- En puntos donde f'(x)Â² â‰ˆ f(x)Â·f''(x)

### ğŸ’¡ CuÃ¡ndo usar
- Cuando sabes que la raÃ­z tiene **multiplicidad > 1**
- Por ejemplo: f(x) = (x-2)Â², (x-3)Â³, etc.
- Cuando Newton estÃ¡ndar **converge muy lentamente**
- Si puedes calcular f''(x) eficientemente

### ğŸ“ˆ Ejemplo de raÃ­z mÃºltiple
```
f(x) = (x - 2)Â²  tiene raÃ­z doble en x = 2
Newton estÃ¡ndar: convergencia LINEAL
Newton modificado: convergencia CUADRÃTICA
```

---

# CapÃ­tulo 2: Sistemas de Ecuaciones Lineales

Estos mÃ©todos resuelven sistemas de la forma **Ax = b**, donde A es una matriz nÃ—n y buscamos el vector x.

## 7. MÃ©todo de Jacobi

### ğŸ¯ Â¿QuÃ© es?
Un mÃ©todo **iterativo** que resuelve sistemas lineales grandes. Actualiza todas las componentes de x **simultÃ¡neamente** en cada iteraciÃ³n.

### ğŸ“Š Â¿CÃ³mo funciona?
1. **DescomposiciÃ³n**: A = D + L + U
   - D = diagonal de A
   - L = parte triangular inferior estricta
   - U = parte triangular superior estricta
2. **IteraciÃ³n**: x^(k+1) = D^(-1)(b - (L+U)x^(k))
3. **Componente a componente**:
   ```
   x_i^(k+1) = (b_i - Î£(a_ijÂ·x_j^(k))) / a_ii
   ```

### âœ… Ventajas
- **Simple de implementar** y paralelizable
- **Requiere poca memoria** (solo almacena dos vectores)
- Funciona bien para matrices **diagonalmente dominantes**
- Puede manejar sistemas **muy grandes**

### âŒ Desventajas
- **Convergencia lenta**
- **No siempre converge**
- Generalmente **mÃ¡s lento que Gauss-Seidel**
- Requiere muchas iteraciones para alta precisiÃ³n

### ğŸ”´ CuÃ¡ndo falla
- Si A **NO es diagonalmente dominante** â†’ puede divergir
- Si el **radio espectral â‰¥ 1** â†’ diverge
- Con matrices **mal condicionadas**

### ğŸ’¡ CuÃ¡ndo usar
- Para matrices **diagonalmente dominantes**:
  - |a_ii| > Î£|a_ij| para todo i
- Cuando quieres **paralelizar** el cÃ¡lculo
- Para sistemas **grandes** y **dispersos** (sparse)
- Como **precondicionador** para otros mÃ©todos

### ğŸ“ˆ Criterio de convergencia
```
Radio espectral Ï(T_jacobi) < 1 â†’ CONVERGE

Diagonalmente dominante â†’ GARANTIZA convergencia
```

---

## 8. MÃ©todo de Gauss-Seidel

### ğŸ¯ Â¿QuÃ© es?
Similar a Jacobi, pero usa los valores **mÃ¡s actualizados** inmediatamente en cada iteraciÃ³n.

### ğŸ“Š Â¿CÃ³mo funciona?
1. **Diferencia clave**: Cuando calculas x_i^(k+1), usas:
   - Los valores **nuevos** x_1^(k+1), ..., x_(i-1)^(k+1) (ya calculados)
   - Los valores **viejos** x_(i+1)^(k), ..., x_n^(k)
2. **FÃ³rmula**:
   ```
   x_i^(k+1) = (b_i - Î£(j<i) a_ijÂ·x_j^(k+1) - Î£(j>i) a_ijÂ·x_j^(k)) / a_ii
   ```

### âœ… Ventajas
- **Generalmente 2x mÃ¡s rÃ¡pido** que Jacobi
- **Mejor convergencia** en la mayorÃ­a de casos
- **Requiere menos memoria** (un solo vector)
- MÃ¡s eficiente que Jacobi

### âŒ Desventajas
- **No es paralelizable** (cÃ¡lculos secuenciales)
- Sigue sin garantizar convergencia en todos los casos
- Puede ser mÃ¡s lento que mÃ©todos directos para sistemas pequeÃ±os

### ğŸ”´ CuÃ¡ndo falla
- Mismas condiciones que Jacobi
- Aunque converge en **mÃ¡s casos** que Jacobi

### ğŸ’¡ CuÃ¡ndo usar
- Mismos casos que Jacobi, pero **preferiblemente**
- Cuando el **orden de las ecuaciones** es secuencial
- Si no necesitas paralelizaciÃ³n
- Como **mejora sobre Jacobi**

### ğŸ“ˆ ComparaciÃ³n Jacobi vs Gauss-Seidel
```
Jacobi:       x_i^(k+1) usa TODOS los valores viejos x^(k)
Gauss-Seidel: x_i^(k+1) usa valores nuevos cuando estÃ¡n disponibles

Resultado: Gauss-Seidel â‰ˆ 2x mÃ¡s rÃ¡pido
```

---

## 9. MÃ©todo SOR (Successive Over-Relaxation)

### ğŸ¯ Â¿QuÃ© es?
Una **mejora de Gauss-Seidel** que usa un parÃ¡metro de **relajaciÃ³n** Ï‰ para acelerar la convergencia.

### ğŸ“Š Â¿CÃ³mo funciona?
1. **FÃ³rmula**:
   ```
   x_i^(k+1) = (1-Ï‰)Â·x_i^(k) + Ï‰/a_iiÂ·[b_i - Î£(j<i) a_ijÂ·x_j^(k+1) - Î£(j>i) a_ijÂ·x_j^(k)]
   ```
2. **ParÃ¡metro Ï‰**:
   - Ï‰ = 1: es **exactamente Gauss-Seidel**
   - 1 < Ï‰ < 2: **sobre-relajaciÃ³n** (acelera)
   - 0 < Ï‰ < 1: **sub-relajaciÃ³n** (estabiliza)
3. **Optimal Ï‰**: Existe un Ï‰ Ã³ptimo que minimiza iteraciones

### âœ… Ventajas
- **Mucho mÃ¡s rÃ¡pido** que Gauss-Seidel con Ï‰ Ã³ptimo
- Puede ser **10x o mÃ¡s rÃ¡pido** en algunos casos
- Flexible: ajustas Ï‰ segÃºn el problema

### âŒ Desventajas
- **DifÃ­cil encontrar Ï‰ Ã³ptimo** (depende de la matriz)
- Requiere **experimentaciÃ³n o teorÃ­a** para elegir Ï‰
- Puede **diverger** si Ï‰ estÃ¡ mal elegido
- MÃ¡s complejo que Gauss-Seidel

### ğŸ”´ CuÃ¡ndo falla
- Si **Ï‰ â‰¥ 2 o Ï‰ â‰¤ 0** â†’ siempre diverge
- Con **Ï‰ mal elegido** puede ser peor que Gauss-Seidel

### ğŸ’¡ CuÃ¡ndo usar
- Cuando conoces el **Ï‰ Ã³ptimo** o puedes estimarlo
- Para matrices donde **Gauss-Seidel es lento**
- En problemas de **elementos finitos** o **diferencias finitas**
- Cuando necesitas **mÃ¡xima velocidad** iterativa

### ğŸ“ˆ SelecciÃ³n de Ï‰
```
Ï‰ = 1:       Gauss-Seidel
1 < Ï‰ < 2:   Acelera (pero puede inestabilizar)
Ï‰ Ã³ptimo:    Depende de Ï(T_GS), matriz especÃ­fica

TÃ­picamente: 1.2 â‰¤ Ï‰ â‰¤ 1.9 funciona bien
```

---

# CapÃ­tulo 3: InterpolaciÃ³n

La interpolaciÃ³n construye una funciÃ³n que **pasa exactamente** por un conjunto de puntos dados.

## 10. InterpolaciÃ³n de Vandermonde

### ğŸ¯ Â¿QuÃ© es?
Un mÃ©todo **directo** que resuelve un sistema lineal para encontrar el polinomio interpolante.

### ğŸ“Š Â¿CÃ³mo funciona?
1. Dados n+1 puntos (xâ‚€,yâ‚€), ..., (xâ‚™,yâ‚™)
2. Busca P(x) = aâ‚€ + aâ‚x + aâ‚‚xÂ² + ... + aâ‚™xâ¿
3. **Matriz de Vandermonde**:
   ```
   [1  xâ‚€  xâ‚€Â²  ...  xâ‚€â¿] [aâ‚€]   [yâ‚€]
   [1  xâ‚  xâ‚Â²  ...  xâ‚â¿] [aâ‚]   [yâ‚]
   [â‹®   â‹®   â‹®   â‹±    â‹® ] [â‹® ] = [â‹® ]
   [1  xâ‚™  xâ‚™Â²  ...  xâ‚™â¿] [aâ‚™]   [yâ‚™]
   ```
4. Resuelve el sistema para obtener los coeficientes

### âœ… Ventajas
- **Conceptualmente simple**
- **Directo**: no requiere iteraciones
- Da los **coeficientes** del polinomio explÃ­citamente

### âŒ Desventajas
- **NumÃ©ricamente inestable** con muchos puntos
- La matriz de Vandermonde estÃ¡ **muy mal condicionada**
- **Muy costoso**: O(nÂ³)
- **No se usa en la prÃ¡ctica** (existen mejores mÃ©todos)

### ğŸ”´ CuÃ¡ndo falla
- Con **muchos puntos** (n > 10-15)
- Cuando los puntos xáµ¢ estÃ¡n **muy juntos** o **muy separados**
- Por **errores de redondeo** acumulados

### ğŸ’¡ CuÃ¡ndo usar
- **CASI NUNCA** en aplicaciones reales
- Solo para **pocos puntos** (n < 5)
- Con fines **educativos** o **teÃ³ricos**
- Mejor usar: **Lagrange o Newton**

---

## 11. InterpolaciÃ³n de Newton (Diferencias Divididas)

### ğŸ¯ Â¿QuÃ© es?
Construye el polinomio interpolante usando **diferencias divididas**. Permite **agregar puntos fÃ¡cilmente**.

### ğŸ“Š Â¿CÃ³mo funciona?
1. **Forma de Newton**:
   ```
   P(x) = f[xâ‚€] + f[xâ‚€,xâ‚](x-xâ‚€) + f[xâ‚€,xâ‚,xâ‚‚](x-xâ‚€)(x-xâ‚) + ...
   ```
2. **Diferencias divididas**:
   ```
   f[xáµ¢] = yáµ¢
   f[xáµ¢,xáµ¢â‚Šâ‚] = (f[xáµ¢â‚Šâ‚] - f[xáµ¢]) / (xáµ¢â‚Šâ‚ - xáµ¢)
   f[xáµ¢,xáµ¢â‚Šâ‚,xáµ¢â‚Šâ‚‚] = (f[xáµ¢â‚Šâ‚,xáµ¢â‚Šâ‚‚] - f[xáµ¢,xáµ¢â‚Šâ‚]) / (xáµ¢â‚Šâ‚‚ - xáµ¢)
   ```
3. Se construye una **tabla de diferencias divididas**

### âœ… Ventajas
- **NumÃ©ricamente estable**
- **Eficiente**: O(nÂ²)
- **FÃ¡cil agregar puntos** sin recalcular todo
- **Mejor que Vandermonde** en todos los aspectos

### âŒ Desventajas
- EvaluaciÃ³n puede ser mÃ¡s compleja que Lagrange
- Necesita **orden de los puntos**

### ğŸ”´ CuÃ¡ndo falla
- Raras veces falla (muy estable)
- Problema general: **oscilaciones** con muchos puntos (fenÃ³meno de Runge)

### ğŸ’¡ CuÃ¡ndo usar
- Cuando **agregarÃ¡s puntos incrementalmente**
- Para **interpolaciÃ³n polinomial** en general
- Cuando quieres **eficiencia y estabilidad**
- Preferible a Vandermonde

---

## 12. InterpolaciÃ³n de Lagrange

### ğŸ¯ Â¿QuÃ© es?
Expresa el polinomio interpolante como suma de **polinomios base de Lagrange**.

### ğŸ“Š Â¿CÃ³mo funciona?
1. **Forma de Lagrange**:
   ```
   P(x) = Î£ yáµ¢ Â· Láµ¢(x)
   ```
2. **Polinomios base**:
   ```
   Láµ¢(x) = Î (jâ‰ i) (x - xâ±¼)/(xáµ¢ - xâ±¼)
   ```
3. Cada Láµ¢(x) vale 1 en xáµ¢ y 0 en los demÃ¡s puntos

### âœ… Ventajas
- **Muy elegante** matemÃ¡ticamente
- **No requiere resolver sistema**
- **Orden de puntos no importa**
- FÃ¡cil de entender conceptualmente

### âŒ Desventajas
- **Ineficiente** para agregar puntos (recalcula todo)
- **EvaluaciÃ³n costosa**: O(nÂ²)
- No reutiliza cÃ¡lculos previos

### ğŸ”´ CuÃ¡ndo falla
- Mismo problema que Newton: **oscilaciones de Runge**

### ğŸ’¡ CuÃ¡ndo usar
- Para **conjuntos fijos** de puntos
- Con **pocos puntos**
- Cuando la **simplicidad teÃ³rica** es importante
- En **ejemplos educativos**

### ğŸ“ˆ ComparaciÃ³n Newton vs Lagrange
```
Newton:   Mejor para CONSTRUIR incrementalmente
Lagrange: Mejor para EVALUAR una vez

Ambos dan el MISMO polinomio
```

---

## 13. Spline Lineal

### ğŸ¯ Â¿QuÃ© es?
En lugar de UN polinomio de grado alto, usa **segmentos de lÃ­neas rectas** entre puntos consecutivos.

### ğŸ“Š Â¿CÃ³mo funciona?
1. Entre cada par de puntos (xáµ¢, yáµ¢) y (xáµ¢â‚Šâ‚, yáµ¢â‚Šâ‚):
   ```
   Sáµ¢(x) = yáµ¢ + (yáµ¢â‚Šâ‚ - yáµ¢)/(xáµ¢â‚Šâ‚ - xáµ¢) Â· (x - xáµ¢)
   ```
2. Es simplemente **interpolaciÃ³n lineal** por segmentos
3. Continuo pero **no suave** (cambios bruscos de pendiente)

### âœ… Ventajas
- **Extremadamente simple**
- **Nunca oscila** (a diferencia de polinomios altos)
- **Muy estable** numÃ©ricamente
- **FÃ¡cil de calcular y evaluar**

### âŒ Desventajas
- **No es suave**: tiene esquinas
- No es diferenciable en los puntos de datos
- **AproximaciÃ³n pobre** para funciones curvas

### ğŸ”´ CuÃ¡ndo falla
- Cuando necesitas **suavidad** (derivadas continuas)
- Para funciones **muy curvas**

### ğŸ’¡ CuÃ¡ndo usar
- Cuando tienes **muchos puntos** cercanos
- Para **visualizaciÃ³n rÃ¡pida**
- Cuando la **simplicidad** es crÃ­tica
- Como **aproximaciÃ³n inicial**

---

## 14. Spline CÃºbico

### ğŸ¯ Â¿QuÃ© es?
Usa **polinomios cÃºbicos** (grado 3) entre cada par de puntos, con condiciones de **suavidad**.

### ğŸ“Š Â¿CÃ³mo funciona?
1. Entre cada par de puntos, usa:
   ```
   Sáµ¢(x) = aáµ¢ + báµ¢(x-xáµ¢) + cáµ¢(x-xáµ¢)Â² + dáµ¢(x-xáµ¢)Â³
   ```
2. **Condiciones**:
   - Pasa por los puntos: Sáµ¢(xáµ¢) = yáµ¢
   - **Continuo**: Sáµ¢(xáµ¢â‚Šâ‚) = Sáµ¢â‚Šâ‚(xáµ¢â‚Šâ‚)
   - **Primera derivada continua**: S'áµ¢(xáµ¢â‚Šâ‚) = S'áµ¢â‚Šâ‚(xáµ¢â‚Šâ‚)
   - **Segunda derivada continua**: S''áµ¢(xáµ¢â‚Šâ‚) = S''áµ¢â‚Šâ‚(xáµ¢â‚Šâ‚)
3. Condiciones de frontera (natural, sujeto, etc.)

### âœ… Ventajas
- **Muy suave**: primera y segunda derivadas continuas
- **No oscila** como polinomios de alto grado
- **Aspecto natural** (minimiza curvatura)
- **EstÃ¡ndar industrial** para interpolaciÃ³n

### âŒ Desventajas
- **MÃ¡s complejo** de implementar
- Requiere **resolver sistema tridiagonal**
- MÃ¡s costoso que spline lineal

### ğŸ”´ CuÃ¡ndo falla
- Raras veces falla
- Puede tener **overshoot** en cambios bruscos de datos

### ğŸ’¡ CuÃ¡ndo usar
- **CASI SIEMPRE** para interpolaciÃ³n en la prÃ¡ctica
- En **grÃ¡ficos por computadora**
- Para **curvas suaves**
- En **CAD/CAM**
- Cuando necesitas **derivadas continuas**

### ğŸ“ˆ Tipos de condiciones de frontera
```
Natural:  S''(xâ‚€) = S''(xâ‚™) = 0
Sujeto:   S'(xâ‚€) y S'(xâ‚™) especificados
Not-a-knot: S'''áµ¢ continua en xâ‚ y xâ‚™â‚‹â‚
```

---

# ComparaciÃ³n de MÃ©todos

## Ecuaciones No Lineales

| MÃ©todo | Convergencia | Derivadas | Robustez | Velocidad | CuÃ¡ndo usar |
|--------|--------------|-----------|----------|-----------|-------------|
| **BisecciÃ³n** | Lineal | No | â­â­â­â­â­ | â­â­ | MÃ¡xima seguridad |
| **Regla Falsa** | Superlineal | No | â­â­â­â­ | â­â­â­ | MÃ¡s rÃ¡pido que bisecciÃ³n |
| **Punto Fijo** | Lineal | No (f') | â­â­ | â­â­â­ | Casos especÃ­ficos |
| **Newton** | CuadrÃ¡tica | SÃ­ (f') | â­â­â­ | â­â­â­â­â­ | MÃ¡xima velocidad |
| **Secante** | Superlineal | No | â­â­â­ | â­â­â­â­ | Sin derivadas, rÃ¡pido |
| **RaÃ­ces MÃºltiples** | CuadrÃ¡tica | SÃ­ (f', f'') | â­â­ | â­â­â­â­ | RaÃ­ces mÃºltiples |

## Sistemas Lineales

| MÃ©todo | Iteraciones | Memoria | Paralelizable | CuÃ¡ndo usar |
|--------|-------------|---------|---------------|-------------|
| **Jacobi** | Muchas | Baja | SÃ­ | Matrices grandes, paralelo |
| **Gauss-Seidel** | Menos | Muy baja | No | Mejor que Jacobi |
| **SOR** | Pocas | Muy baja | No | MÃ¡xima velocidad iterativa |

## InterpolaciÃ³n

| MÃ©todo | Estabilidad | Suavidad | Oscilaciones | CuÃ¡ndo usar |
|--------|-------------|----------|--------------|-------------|
| **Vandermonde** | â­ | Alta | Altas | NUNCA |
| **Newton** | â­â­â­â­ | Alta | Altas (muchos pts) | ConstrucciÃ³n incremental |
| **Lagrange** | â­â­â­â­ | Alta | Altas (muchos pts) | Pocos puntos |
| **Spline Lineal** | â­â­â­â­â­ | Baja | Ninguna | Simple, muchos puntos |
| **Spline CÃºbico** | â­â­â­â­â­ | Muy alta | Ninguna | **PREFERIDO** |

---

## ğŸ“ Consejos Generales

### Para Ecuaciones No Lineales:
1. **Empieza con bisecciÃ³n** para acotar la raÃ­z
2. **Usa Newton** si tienes buena aproximaciÃ³n inicial
3. **Usa Secante** si no puedes calcular derivadas
4. **Verifica convergencia** en cada iteraciÃ³n

### Para Sistemas Lineales:
1. **Verifica diagonal dominancia** primero
2. **Usa Gauss-Seidel** como estÃ¡ndar
3. **Prueba SOR** si necesitas velocidad
4. **Considera mÃ©todos directos** para sistemas pequeÃ±os

### Para InterpolaciÃ³n:
1. **Usa Spline CÃºbico** casi siempre
2. **Evita polinomios de grado > 10**
3. **Ten cuidado con el fenÃ³meno de Runge**
4. **Considera aproximaciÃ³n** en lugar de interpolaciÃ³n exacta

---

## ğŸ“š Referencias y Lectura Adicional

- Burden & Faires: "Numerical Analysis"
- Chapra & Canale: "Numerical Methods for Engineers"
- Press et al.: "Numerical Recipes"
- Wikipedia: artÃ­culos sobre cada mÃ©todo

---

**Autor**: MethodLab Project  
**Fecha**: Noviembre 2025  
**VersiÃ³n**: 1.0
